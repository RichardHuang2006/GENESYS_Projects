{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31017e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed8488a3883459d915a7d9e8a5f9306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\huang\\.cache\\huggingface\\hub\\models--timm--vit_tiny_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.5286, Train Acc = 0.4713, Val Loss = 1.2457, Val Acc = 0.4318\n",
      "Epoch 2: Train Loss = 1.0240, Train Acc = 0.5920, Val Loss = 1.0499, Val Acc = 0.5909\n",
      "Epoch 3: Train Loss = 0.7889, Train Acc = 0.6782, Val Loss = 1.2054, Val Acc = 0.6136\n",
      "Epoch 4: Train Loss = 0.5094, Train Acc = 0.7874, Val Loss = 1.2610, Val Acc = 0.5682\n",
      "Epoch 5: Train Loss = 0.3358, Train Acc = 0.8736, Val Loss = 1.6784, Val Acc = 0.5000\n",
      "Epoch 6: Train Loss = 0.2269, Train Acc = 0.9253, Val Loss = 1.9426, Val Acc = 0.4318\n",
      "Epoch 7: Train Loss = 0.1126, Train Acc = 0.9713, Val Loss = 1.8374, Val Acc = 0.5000\n",
      "Epoch 8: Train Loss = 0.0446, Train Acc = 1.0000, Val Loss = 1.8736, Val Acc = 0.5682\n",
      "Epoch 9: Train Loss = 0.0215, Train Acc = 1.0000, Val Loss = 1.9981, Val Acc = 0.5682\n",
      "Epoch 10: Train Loss = 0.0113, Train Acc = 1.0000, Val Loss = 2.0672, Val Acc = 0.5227\n",
      "Epoch 11: Train Loss = 0.0085, Train Acc = 1.0000, Val Loss = 2.0660, Val Acc = 0.5682\n",
      "Epoch 12: Train Loss = 0.0046, Train Acc = 1.0000, Val Loss = 2.0994, Val Acc = 0.5909\n",
      "Epoch 13: Train Loss = 0.0042, Train Acc = 1.0000, Val Loss = 2.1115, Val Acc = 0.5909\n",
      "Epoch 14: Train Loss = 0.0040, Train Acc = 1.0000, Val Loss = 2.1189, Val Acc = 0.5909\n",
      "Epoch 15: Train Loss = 0.0038, Train Acc = 1.0000, Val Loss = 2.1254, Val Acc = 0.5909\n",
      "Epoch 16: Train Loss = 0.0036, Train Acc = 1.0000, Val Loss = 2.1288, Val Acc = 0.5909\n",
      "Epoch 17: Train Loss = 0.0035, Train Acc = 1.0000, Val Loss = 2.1321, Val Acc = 0.5909\n",
      "Epoch 18: Train Loss = 0.0035, Train Acc = 1.0000, Val Loss = 2.1356, Val Acc = 0.5909\n",
      "Epoch 19: Train Loss = 0.0034, Train Acc = 1.0000, Val Loss = 2.1375, Val Acc = 0.5909\n",
      "Epoch 20: Train Loss = 0.0034, Train Acc = 1.0000, Val Loss = 2.1393, Val Acc = 0.5909\n",
      "Model weights saved to vit_tiny_5class.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import timm  # PyTorch Image Models\n",
    "\n",
    "# --------------------- Dataset ---------------------\n",
    "\n",
    "class ImageArrayDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, npy_path = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = int(np.argmax(np.load(npy_path)))\n",
    "        return image, label\n",
    "\n",
    "# --------------------- Training ---------------------\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        val_loss /= total\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, \"\n",
    "              f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# --------------------- Main ---------------------\n",
    "\n",
    "def main():\n",
    "    folder_path = r\"C:\\Users\\huang\\Downloads\\Engineering Projects\\Genesys Lab\\v5\"\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    val_split = 0.2\n",
    "    seed = 42\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 1. Collect samples\n",
    "    all_samples = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.jpg') or fname.endswith('.png'):\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            img_path = os.path.join(folder_path, fname)\n",
    "            npy_path = os.path.join(folder_path, base + '.npy')\n",
    "            if os.path.exists(npy_path):\n",
    "                all_samples.append((img_path, npy_path))\n",
    "\n",
    "    random.shuffle(all_samples)\n",
    "    split_idx = int(len(all_samples) * (1 - val_split))\n",
    "    train_samples = all_samples[:split_idx]\n",
    "    val_samples = all_samples[split_idx:]\n",
    "\n",
    "    # 2. Transforms with ImageNet normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Keep original ViT input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                             std=[0.229, 0.224, 0.225]),  # ImageNet std\n",
    "    ])\n",
    "\n",
    "    train_dataset = ImageArrayDataset(train_samples, transform=transform)\n",
    "    val_dataset = ImageArrayDataset(val_samples, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3. Load smaller pretrained ViT for faster training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=5).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "\n",
    "    train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs)\n",
    "    torch.save(model.state_dict(), 'vit_tiny_5class.pth')\n",
    "    print(\"Model weights saved to vit_tiny_5class.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bf3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
